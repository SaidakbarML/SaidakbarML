{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "# Introduction: Enhancing Image Accessibility for the Visually Impaired through Caption Generation\n",
    "\n",
    "In a world saturated with visual information, the ability to interpret and understand images is a fundamental aspect of daily life. However, for individuals with visual impairments, this experience is often limited. The advancement of technology offers a transformative opportunity to bridge this gap, empowering the visually impaired community with tools that can convey the visual world through alternative means.\n",
    "\n",
    "This project centers around the task of image captioning, a process where we aim to create a deep learning model capable of generating descriptive captions for images. Leveraging the power of Convolutional Neural Networks (CNNs) for image feature extraction and Long Short-Term Memory networks (LSTMs) for sequence-to-sequence tasks, our objective is to enable a machine to \"understand\" the contents of an image and articulate it in a form accessible to all.\n",
    "\n",
    "The dataset at the core of our exploration is the Flickr8K dataset, a rich collection of images accompanied by human-generated captions. By utilizing this dataset, we embark on a journey to develop a model that not only captures the intricate details within an image but also employs an attention mechanism to refine the caption generation process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color:white;\n",
    "           display:fill;\n",
    "           border-radius:5px;\n",
    "           background-color:red;\n",
    "           font-family:Verdana;\n",
    "           letter-spacing:0.5px;\n",
    "           text-align:center;\">\n",
    "  <p style=\"padding: 10px;\n",
    "            color:white;\n",
    "            margin: 0;\n",
    "            font-size: 130%;\">\n",
    "   Flickr8K Image Captioning \n",
    "  </p>\n",
    "</div>\n",
    "<ol>\n",
    "\n",
    " <li><a href=\"#1\">IMPORTNING LIBRARIES AND FRAMEWORKS</a></li>\n",
    "  <li><a href=\"#2\">IMAGE CAPTIONING </a></li>\n",
    "  <li><a href=\"#3\">VISUALIZATION</a></li>\n",
    "  <li><a href=\"#4\">CAPTION TEXT PROCESSING </a></li>\n",
    "  <li><a href=\"#5\">IMAGE FEATURE EXTRACTION </a></li>\n",
    " \n",
    "  <li><a href=\"#6\">DATA GENERATION</a></li>\n",
    "  <li><a href=\"#7\">MODELLING</a></li>\n",
    "  <li><a href=\"#8\">TRAINIG</a></li>\n",
    "  <li><a href=\"#9\">RESULTS</a></li>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id = 1 style=\"color:white;\n",
    "           display:fill;\n",
    "           border-radius:5px;\n",
    "           background-color:red;\n",
    "           font-family:Verdana;\n",
    "           letter-spacing:0.5px;\n",
    "           text-align:center;\">\n",
    "  <p style=\"padding: 10px;\n",
    "            color:white;\n",
    "            margin: 0;\n",
    "            font-size: 130%;\">\n",
    "IMPORTNING LIBRARIES AND FRAMEWORKS  </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import Sequence\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D, Activation, Dropout, Flatten, Dense, Input, Layer\n",
    "from tensorflow.keras.layers import Embedding, LSTM, add, Concatenate, Reshape, concatenate, Bidirectional\n",
    "from tensorflow.keras.applications import VGG16, ResNet50, DenseNet201\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from textwrap import wrap\n",
    "\n",
    "plt.rcParams['font.size'] = 12\n",
    "sns.set_style(\"dark\")\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id = 2 style=\"color:white;\n",
    "           display:fill;\n",
    "           border-radius:5px;\n",
    "           background-color:red;\n",
    "           font-family:Verdana;\n",
    "           letter-spacing:0.5px;\n",
    "           text-align:center;\">\n",
    "  <p style=\"padding: 10px;\n",
    "            color:white;\n",
    "            margin: 0;\n",
    "            font-size: 130%;\">\n",
    "  IMAGE CAPTIONING </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = 'path to data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"path flickr8k/captions.txt\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readImage(path,img_size=224):\n",
    "    img = load_img(path,color_mode='rgb',target_size=(img_size,img_size))\n",
    "    img = img_to_array(img)\n",
    "    img = img/255.\n",
    "    \n",
    "    return img\n",
    "\n",
    "def display_images(temp_df):\n",
    "    temp_df = temp_df.reset_index(drop=True)\n",
    "    plt.figure(figsize = (20 , 20))\n",
    "    n = 0\n",
    "    for i in range(15):\n",
    "        n+=1\n",
    "        plt.subplot(5 , 5, n)\n",
    "        plt.subplots_adjust(hspace = 0.7, wspace = 0.3)\n",
    "        image = readImage(f\"../input/flickr8k/Images/{temp_df.image[i]}\")\n",
    "        plt.imshow(image)\n",
    "        plt.title(\"\\n\".join(wrap(temp_df.caption[i], 20)))\n",
    "        plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id = 3 style=\"color:white;\n",
    "           display:fill;\n",
    "           border-radius:5px;\n",
    "           background-color:red;\n",
    "           font-family:Verdana;\n",
    "           letter-spacing:0.5px;\n",
    "           text-align:center;\">\n",
    "  <p style=\"padding: 10px;\n",
    "            color:white;\n",
    "            margin: 0;\n",
    "            font-size: 130%;\">\n",
    "  VISUALIZATION</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_images(data.sample(15))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id = 4 style=\"color:white;\n",
    "           display:fill;\n",
    "           border-radius:5px;\n",
    "           background-color:red;\n",
    "           font-family:Verdana;\n",
    "           letter-spacing:0.5px;\n",
    "           text-align:center;\">\n",
    "  <p style=\"padding: 10px;\n",
    "            color:white;\n",
    "            margin: 0;\n",
    "            font-size: 130%;\">\n",
    " CAPTION TEXT PROCESSING </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_preprocessing(data):\n",
    "    data['caption'] = data['caption'].apply(lambda x: x.lower())\n",
    "    data['caption'] = data['caption'].apply(lambda x: x.replace(\"[^A-Za-z]\",\"\"))\n",
    "    data['caption'] = data['caption'].apply(lambda x: x.replace(\"\\s+\",\" \"))\n",
    "    data['caption'] = data['caption'].apply(lambda x: \" \".join([word for word in x.split() if len(word)>1]))\n",
    "    data['caption'] = \"startseq \"+data['caption']+\" endseq\"\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = text_preprocessing(data)\n",
    "captions = data['caption'].tolist()\n",
    "captions[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(captions)\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "max_length = max(len(caption.split()) for caption in captions)\n",
    "\n",
    "images = data['image'].unique().tolist()\n",
    "nimages = len(images)\n",
    "\n",
    "split_index = round(0.85*nimages)\n",
    "train_images = images[:split_index]\n",
    "val_images = images[split_index:]\n",
    "\n",
    "train = data[data['image'].isin(train_images)]\n",
    "test = data[data['image'].isin(val_images)]\n",
    "\n",
    "train.reset_index(inplace=True,drop=True)\n",
    "test.reset_index(inplace=True,drop=True)\n",
    "\n",
    "tokenizer.texts_to_sequences([captions[1]])[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id = 5 style=\"color:white;\n",
    "           display:fill;\n",
    "           border-radius:5px;\n",
    "           background-color:red;\n",
    "           font-family:Verdana;\n",
    "           letter-spacing:0.5px;\n",
    "           text-align:center;\">\n",
    "  <p style=\"padding: 10px;\n",
    "            color:white;\n",
    "            margin: 0;\n",
    "            font-size: 130%;\">\n",
    " IMAGE FEATURE EXTRACTION</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DenseNet201()\n",
    "fe = Model(inputs=model.input, outputs=model.layers[-2].output)\n",
    "\n",
    "img_size = 224\n",
    "features = {}\n",
    "for image in tqdm(data['image'].unique().tolist()):\n",
    "    img = load_img(os.path.join(image_path,image),target_size=(img_size,img_size))\n",
    "    img = img_to_array(img)\n",
    "    img = img/255.\n",
    "    img = np.expand_dims(img,axis=0)\n",
    "    feature = fe.predict(img, verbose=0)\n",
    "    features[image] = feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id = 6 style=\"color:white;\n",
    "           display:fill;\n",
    "           border-radius:5px;\n",
    "           background-color:red;\n",
    "           font-family:Verdana;\n",
    "           letter-spacing:0.5px;\n",
    "           text-align:center;\">\n",
    "  <p style=\"padding: 10px;\n",
    "            color:white;\n",
    "            margin: 0;\n",
    "            font-size: 130%;\">\n",
    "  DATA GENERATION</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataGenerator(Sequence):\n",
    "    \n",
    "    def __init__(self, df, X_col, y_col, batch_size, directory, tokenizer, \n",
    "                 vocab_size, max_length, features,shuffle=True):\n",
    "    \n",
    "        self.df = df.copy()\n",
    "        self.X_col = X_col\n",
    "        self.y_col = y_col\n",
    "        self.directory = directory\n",
    "        self.batch_size = batch_size\n",
    "        self.tokenizer = tokenizer\n",
    "        self.vocab_size = vocab_size\n",
    "        self.max_length = max_length\n",
    "        self.features = features\n",
    "        self.shuffle = shuffle\n",
    "        self.n = len(self.df)\n",
    "        \n",
    "    def on_epoch_end(self):\n",
    "        if self.shuffle:\n",
    "            self.df = self.df.sample(frac=1).reset_index(drop=True)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.n // self.batch_size\n",
    "    \n",
    "    def __getitem__(self,index):\n",
    "    \n",
    "        batch = self.df.iloc[index * self.batch_size:(index + 1) * self.batch_size,:]\n",
    "        X1, X2, y = self.__get_data(batch)        \n",
    "        return (X1, X2), y\n",
    "    \n",
    "    def __get_data(self,batch):\n",
    "        \n",
    "        X1, X2, y = list(), list(), list()\n",
    "        \n",
    "        images = batch[self.X_col].tolist()\n",
    "           \n",
    "        for image in images:\n",
    "            feature = self.features[image][0]\n",
    "            \n",
    "            captions = batch.loc[batch[self.X_col]==image, self.y_col].tolist()\n",
    "            for caption in captions:\n",
    "                seq = self.tokenizer.texts_to_sequences([caption])[0]\n",
    "\n",
    "                for i in range(1,len(seq)):\n",
    "                    in_seq, out_seq = seq[:i], seq[i]\n",
    "                    in_seq = pad_sequences([in_seq], maxlen=self.max_length)[0]\n",
    "                    out_seq = to_categorical([out_seq], num_classes=self.vocab_size)[0]\n",
    "                    X1.append(feature)\n",
    "                    X2.append(in_seq)\n",
    "                    y.append(out_seq)\n",
    "            \n",
    "        X1, X2, y = np.array(X1), np.array(X2), np.array(y)\n",
    "                \n",
    "        return X1, X2, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id = 7 style=\"color:white;\n",
    "           display:fill;\n",
    "           border-radius:5px;\n",
    "           background-color:red;\n",
    "           font-family:Verdana;\n",
    "           letter-spacing:0.5px;\n",
    "           text-align:center;\">\n",
    "  <p style=\"padding: 10px;\n",
    "            color:white;\n",
    "            margin: 0;\n",
    "            font-size: 130%;\">\n",
    " MODELLING </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input1 = Input(shape=(1920,))\n",
    "input2 = Input(shape=(max_length,))\n",
    "\n",
    "img_features = Dense(256, activation='relu')(input1)\n",
    "img_features_reshaped = Reshape((1, 256), input_shape=(256,))(img_features)\n",
    "\n",
    "sentence_features = Embedding(vocab_size, 256, mask_zero=False)(input2)\n",
    "merged = concatenate([img_features_reshaped,sentence_features],axis=1)\n",
    "sentence_features = LSTM(256)(merged)\n",
    "x = Dropout(0.5)(sentence_features)\n",
    "x = add([x, img_features])\n",
    "x = Dense(128, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "output = Dense(vocab_size, activation='softmax')(x)\n",
    "\n",
    "caption_model = Model(inputs=[input1,input2], outputs=output)\n",
    "caption_model.compile(loss='categorical_crossentropy',optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import plot_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(caption_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "caption_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = CustomDataGenerator(df=train,X_col='image',y_col='caption',batch_size=64,directory=image_path,\n",
    "                                      tokenizer=tokenizer,vocab_size=vocab_size,max_length=max_length,features=features)\n",
    "\n",
    "validation_generator = CustomDataGenerator(df=test,X_col='image',y_col='caption',batch_size=64,directory=image_path,\n",
    "                                      tokenizer=tokenizer,vocab_size=vocab_size,max_length=max_length,features=features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"model.h5\"\n",
    "checkpoint = ModelCheckpoint(model_name,\n",
    "                            monitor=\"val_loss\",\n",
    "                            mode=\"min\",\n",
    "                            save_best_only = True,\n",
    "                            verbose=1)\n",
    "\n",
    "earlystopping = EarlyStopping(monitor='val_loss',min_delta = 0, patience = 5, verbose = 1, restore_best_weights=True)\n",
    "\n",
    "learning_rate_reduction = ReduceLROnPlateau(monitor='val_loss', \n",
    "                                            patience=3, \n",
    "                                            verbose=1, \n",
    "                                            factor=0.2, \n",
    "                                            min_lr=0.00000001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id =8 style=\"color:white;\n",
    "           display:fill;\n",
    "           border-radius:5px;\n",
    "           background-color:red;\n",
    "           font-family:Verdana;\n",
    "           letter-spacing:0.5px;\n",
    "           text-align:center;\">\n",
    "  <p style=\"padding: 10px;\n",
    "            color:white;\n",
    "            margin: 0;\n",
    "            font-size: 130%;\">\n",
    "  TRAINGING </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = caption_model.fit(\n",
    "        train_generator,\n",
    "        epochs=50,\n",
    "        validation_data=validation_generator,\n",
    "        callbacks=[checkpoint,earlystopping,learning_rate_reduction])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,8))\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def idx_to_word(integer,tokenizer):\n",
    "    \n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        if index==integer:\n",
    "            return word\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_caption(model, image, tokenizer, max_length, features):\n",
    "    \n",
    "    feature = features[image]\n",
    "    in_text = \"startseq\"\n",
    "    for i in range(max_length):\n",
    "        sequence = tokenizer.texts_to_sequences([in_text])[0]\n",
    "        sequence = pad_sequences([sequence], max_length)\n",
    "\n",
    "        y_pred = model.predict([feature,sequence])\n",
    "        y_pred = np.argmax(y_pred)\n",
    "        \n",
    "        word = idx_to_word(y_pred, tokenizer)\n",
    "        \n",
    "        if word is None:\n",
    "            break\n",
    "            \n",
    "        in_text+= \" \" + word\n",
    "        \n",
    "        if word == 'endseq':\n",
    "            break\n",
    "            \n",
    "    return in_text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = test.sample(10)\n",
    "samples.reset_index(drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index,record in samples.iterrows():\n",
    "\n",
    "    img = load_img(os.path.join(image_path,record['image']),target_size=(224,224))\n",
    "    img = img_to_array(img)\n",
    "    img = img/255.\n",
    "    \n",
    "    caption = predict_caption(caption_model, record['image'], tokenizer, max_length, features)\n",
    "    samples.loc[index,'caption'] = caption"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id = 9 style=\"color:white;\n",
    "           display:fill;\n",
    "           border-radius:5px;\n",
    "           background-color:red;\n",
    "           font-family:Verdana;\n",
    "           letter-spacing:0.5px;\n",
    "           text-align:center;\">\n",
    "  <p style=\"padding: 10px;\n",
    "            color:white;\n",
    "            margin: 0;\n",
    "            font-size: 130%;\">\n",
    "  RESULTS</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_images(samples)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
